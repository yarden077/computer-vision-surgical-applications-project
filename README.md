# computer-vision-surgical-applications-project

This repository contains the full pipeline for developing and evaluating a 2D keypoint detection system for articulated surgical instruments.  
The project is structured into three phases:  

1. **Synthetic Data Generation** â€“ using BlenderProc to create synthetic annotated datasets of surgical tools.  
2. **Model Training & Inference** â€“ training YOLO-Pose models on synthetic data and running inference on real surgical videos.  
3. **Domain Adaptation / Refinement** â€“ refining the model on unlabeled real surgical videos using pseudo-labeling and self-training.

---

## ðŸ“¦ Environment Setup

Clone the repository:

```bash
git clone https://github.com/your-username/computer-vision-surgical-applications-project.git
cd computer-vision-surgical-applications-project
```
## Install dependencies:
```bash
pip install -r requirements.txt
```
## Reproducing Results
The workflow is divided into 4 main steps:

1. Synthetic Data Generation

Run the BlenderProc script to generate synthetic datasets with annotations:
```bash
python synthetic_data_generator.py
```
2. Training on Synthetic Data
Train YOLO-Pose on the generated dataset:
```bash
yolo pose train model=yolov8m-pose.pt data=your_dataset.yaml epochs=100 imgsz=640
```
3. Refinement with Real Unlabeled Data
Use pseudo-labeling to adapt the synthetic-trained model to real surgical videos:
```bash
python pseudo_label.py
```
This will generate pseudo-labels for the real dataset and allow you to retrain/refine the model.

4. Inference
	â€¢	On images:
```bash
python predict.py
```
	â€¢	On videos:
 ```bash
 python video.py --source path/to/video.mp4
```
The predictions (bounding boxes + keypoints) will be saved under runs/pose/predict*/.

##  Final Model Weights
	â€¢	Phase 2 (Synthetic-only model): [Download here]()
 


